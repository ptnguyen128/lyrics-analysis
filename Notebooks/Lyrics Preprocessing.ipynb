{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import pickle\n",
    "import string\n",
    "from string import digits\n",
    "import dateutil.parser as parser\n",
    "from datetime import datetime\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk, nltk.stem as stem\n",
    "from nltk.corpus import stopwords, words as nltk_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Lyrics from Genius.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sign up for an account that authorizes Genius API access. They will give you a `client_id` and a `client_secret` to be used as authorization keys.\n",
    "- Install `lyricsgenius` (Python wrapper) by executing `$pip install lyricsgenius`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius as genius\n",
    "my_access_token = '1_cCky2Ywyz79M4_3zuNQrDMQHF8TdLnZ7A7rMivBnMYAyJWvNIxcs0EcazGFirq'\n",
    "api = genius.Genius(my_access_token)\n",
    "artist = api.search_artist('BTS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the titles of all the songs by BTS that were found on Genius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titles of all the songs\n",
    "titles = [s.title for s in artist.songs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genius provides English translations to all the songs, but under the artist \"Genius Translation\", so we need to modify the titles accordingly so that it can be searched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the titles to get their english translations\n",
    "titles_1 = ['BTS - ' + t + ' (English Translation)' for t in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = []\n",
    "for t in titles_1:\n",
    "    song = api.search_song(t, 'Genius Translations')\n",
    "    if song not in songs:\n",
    "        songs.append(song)\n",
    "\n",
    "# Only get the songs that are available\n",
    "songs = [s for s in songs if s is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pickle this file so we don't have to do all of these again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the songs\n",
    "song_pickle = open(\"BTS_songs.pickle\", \"wb\")\n",
    "pickle.dump(songs, song_pickle)\n",
    "song_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many songs in total\n",
    "len(songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a Dataframe that consists of necessary information, like Title, Album, Release Year, and Lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = pd.DataFrame(\n",
    "        {'Title': [s.title for s in songs],\n",
    "         'Album': [s.album for s in songs],\n",
    "         'Release': [s.year for s in songs],\n",
    "         'Lyrics': [s.lyrics for s in songs]\n",
    "        } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the titles\n",
    "Just to make our lives easier; we will not be using Titles for analysis so this doesn't have to be pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    return \"\".join(c for c in text if ord(c)<128)\n",
    "\n",
    "def remove_substr(text):\n",
    "    pattern1 = 'BTS\\s-\\s'\n",
    "    pattern2 = '\\s\\(English\\sTranslation\\)'\n",
    "    pattern3 = '\\s\\[English\\sTranslation\\]'\n",
    "    text = re.sub(pattern1, '', text)\n",
    "    text = re.sub(pattern2, '', text)\n",
    "    text = re.sub(pattern3, '', text)\n",
    "    text = re.sub('\\(\\s+\\)', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in song_df.iterrows():\n",
    "    title = remove_non_ascii(row['Title'])\n",
    "    title = remove_substr(title)\n",
    "    row['Title'] = title.lstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the album titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in song_df.iterrows():\n",
    "    if row['Album'] is None:\n",
    "        row['Album'] = 'Unknown'\n",
    "    else:\n",
    "        title = remove_non_ascii(row['Album'])\n",
    "        title = remove_substr(title)\n",
    "        row['Album'] = title.lstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the lyrics\n",
    "We need to normalize the lyrics, and strip off all the stopwords. There are a lot of words that don't make sense or written in a particular way that need fixing as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_words = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    return [word.lower() for word in words]\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    new_words = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    del_words = ['ay', 'oh', 'yeah', 'la', 'lala', 'lalala', 'lalalala', 'lalalalala',\n",
    "                 'hey', 'heh', 'na', 'ah', 'ye', 'ey', 'woah', 'woo', 'gim', 'wo', 'wow',\n",
    "                   'brr', 'yo', 'yah', 'ya', 'eris', 'th', 'st', 'rd', 'selfreproach',\n",
    "                 'bwahahaha',\n",
    "                  'errday', 'ca', 'nt', 'gon', 'lem',\n",
    "                'panman', 'bang', 'tan', 'nyeon', 'dan', 'ge', 'cuz',\n",
    "                'kang', 'baek', 'ho', 'til', 'jinjim', 'abcdefgh',\n",
    "                'errthing', 'errthang', 'errbody', 'stich', 'sucka', 'betta', 'kaws', 'ahh', 'randa', 'rida',\n",
    "                 'hehe', 'haha', 'imma', 'plaing', 'yeon', 'ai', 'hiphip',\n",
    "                'bu', 'hukhuk', 'uslike', 'wan', 'uh', 'hoo', 'eh', 'ddaeng',\n",
    "                'oneeight', 'onethree', 'threeeight', 'hunnit', 'calmy', 'lalalack',\n",
    "                'hhope', 'accel', 'lilililike', 'lilike', 'bubut', 'illegirl', 'ta', 'bout', 'ayo',\n",
    "                'dunno', 'wus', 'wassup', 'shim', 'chung', 'krsone', 'illmatic',\n",
    "                'eolssu', 'ulsoo', 'ulssu', 'ohohowoah', 'ohohowoahowoah', 'dunkiduk', \n",
    "                'kungduruhruh', 'thang', 'mymy', 'deonggideok', 'kungdeoreoreo', 'em', 'heoeohoh', 'heyheyho',\n",
    "                'jungun', 'im', 'whatchu', 'cmon', 'beging', 'friz', 'milli', 'bwa', 'doo',\n",
    "                'joo', 'ching', 'leggo', 'bgm', 'mma', 'within', 'ilsan', 'mon', 'han', \n",
    "                 'mt', 'mudeung', 'hoshigi', 'dboy', 'dom',\n",
    "                'hugok', 'festa', 'baam', 'lodi', 'dodi', 'pyung', 'monster', \n",
    "                'hurryhurryhurryhurryhurryhurryhurry', 'nanakon', 'ohho', 'nwappwappwappwa',\n",
    "                'taehyung', 'hopehopehopehope', 'jeon', 'jindda', 'lobullshitter', 'soondae',\n",
    "                'yoongi', 'hahahaha', 'ohhh', 'namib', 'ouuuuu', 'aaahahh', 'ahhahahhhhm', 'ctrlc', 'ctrlv',\n",
    "                'dont', 'jklmnopqrst', 'hol', 'holeinone', 'handlib', 'killa', 'illa', 'dat',\n",
    "                'runch', 'tombullshittone']\n",
    "    stop_words += del_words\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    lemmatizer = stem.WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "def stem_verbs(words,p='v'):\n",
    "    stemmer = stem.LancasterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def normalize(words):\n",
    "    #words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def stem_lemmatize(words):\n",
    "    words = lemmatize_words(words)\n",
    "    words = stem_verbs(words)\n",
    "    return words\n",
    "\n",
    "def remove_numbers(words):\n",
    "    return [re.sub('[^a-zA-Z]*','',word) for word in words]\n",
    "\n",
    "def fix_lyrics(words):\n",
    "    patterns = ['fullofregrets', 'stststutter', 'kkeut', 'kiddin', 'rockin', 'rollin',\n",
    "                'gegegetting', 'hiddeni', 'zerointerest', 'fk', 'bs', 'swaggin',\n",
    "                'skool', 'origing', 'knowitall', 'tugofwar', 'coupley', 'jujujujump', 'heroesintraining',\n",
    "               'feeli', 'lastditch', 'feelingng', 'feelingn', 'obullshitcure', 'thumbullshit', 'obullshitcurities',\n",
    "               'rreal', 'lonelily', 'crowtit']\n",
    "    fix = ['regrets', 'stutter', 'end', 'kidding', 'rocking', 'rolling',\n",
    "           'getting', 'hidden', 'zero', 'fuck', 'bullshit', 'swag',\n",
    "           'school', 'origin', 'know-it-all', 'tug-of-war', 'couple', 'jump', 'hero',\n",
    "          'feeling', 'last', 'feeling', 'feeling', 'bullshit', 'bullshit', 'insecurities',\n",
    "          'real', 'lonely', 'crow']\n",
    "    for p,f in zip(patterns,fix):\n",
    "        words = [re.sub(p,f,word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in song_df.iterrows():\n",
    "    lyrics = remove_between_square_brackets(row['Lyrics'])\n",
    "    lyrics = nltk.word_tokenize(lyrics)\n",
    "    lyrics = normalize(lyrics)\n",
    "    lyrics = fix_lyrics(lyrics)\n",
    "    row['Lyrics'] = \" \".join(lyrics).lstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleted duplicated songs (remixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = song_df[song_df.Title.str.contains('[mM]ix') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.head(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting albums\n",
    "Some albums were missing release date so let's add them back using information on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in some missing album release dates\n",
    "for index,row in song_df.iterrows():\n",
    "    if row['Album'] == 'O!RUL8,2?':\n",
    "        row['Release'] = '2013-09-11'\n",
    "    if row['Album'] == 'Dark&Wild':\n",
    "        row['Release'] = '2014-08-19'\n",
    "    if row['Album'] == '2 Cool 4 Skool ':\n",
    "        row['Release'] = '2013-06-12'\n",
    "    if row['Album'] == 'Skool Luv Affair (Special Edition)':\n",
    "        row['Release'] = '2014-02-12'\n",
    "    if row['Album'] == \"Love Yourself  'Answer'\":\n",
    "        row['Release'] = '2018-08-24'\n",
    "        \n",
    "for index,row in song_df.iterrows():\n",
    "    if row['Release'] is None:\n",
    "        row['Release'] = '2016'\n",
    "    else:\n",
    "        row['Release'] = datetime.strptime(row['Release'], '%Y-%m-%d').year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "file_name = 'BTS_lyrics.csv'\n",
    "song_df.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
